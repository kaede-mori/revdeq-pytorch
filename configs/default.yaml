# RevDEQ Training Configuration

# Model configuration
hidden_size: 768
num_layers: 12
num_heads: 12
intermediate_size: 3072
dropout: 0.1
max_position_embeddings: 512
vocab_size: 50257
num_fixed_point_iterations: 10
fixed_point_tol: 1e-5
use_reversible: true
beta: 0.8  # Relaxation parameter for reversible updates

# Training configuration
num_epochs: 3
batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 0.00005  # 5e-5 (YAML doesn't handle scientific notation well)
weight_decay: 0.01
warmup_steps: 1000
logging_steps: 100
save_steps: 1000
save_total_limit: 3
fp16: true
bf16: false
dataloader_num_workers: 4
report_to: "none"  # Use "tensorboard" if model.config.to_json_string() is implemented

# Tokenizer
tokenizer: "gpt2"

