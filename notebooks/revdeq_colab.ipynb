{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RevDEQå®Ÿé¨“ - Loss 25ã¾ã§ä¸‹ãŒã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€RevDEQãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€lossãŒ25ç¨‹åº¦ã¾ã§ä¸‹ãŒã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# GitHubãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³\n",
    "!git clone https://github.com/kaede-mori/revdeq-pytorch.git\n",
    "%cd revdeq-pytorch\n",
    "\n",
    "# ãƒ‘ã‚¹ã‚’è¿½åŠ \n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Repository cloned successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from revdeq import RevDEQ, RevDEQConfig\n",
    "from train import prepare_dataset, RevDEQDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šï¼ˆloss 25ã‚’ç›®æ¨™ã«æœ€é©åŒ–ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "%pip install -q torch transformers datasets accelerate tqdm wandb pyyaml tensorboard\n",
    "\n",
    "# GPUç¢ºèª\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WikiText-2ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "print(\"Loading WikiText-2 dataset...\")\n",
    "texts = prepare_dataset(\"wikitext\", \"wikitext-2-raw-v1\", max_texts=None)  # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨\n",
    "\n",
    "print(f\"Loaded {len(texts)} text examples\")\n",
    "print(f\"Sample text: {texts[0][:100]}...\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\n",
    "train_dataset = RevDEQDataset(texts, tokenizer, max_length=model_config.max_position_embeddings)\n",
    "print(f\"Dataset size: {len(train_dataset)} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å­¦ç¿’è¨­å®šï¼ˆloss 25ã‚’ç›®æŒ‡ã™æœ€é©åŒ–ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 25ã‚’ç›®æŒ‡ã™ãŸã‚ã®å­¦ç¿’è¨­å®š\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=5,  # è¤‡æ•°ã‚¨ãƒãƒƒã‚¯ã§å­¦ç¿’\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,  # å®Ÿè³ªçš„ãªãƒãƒƒãƒã‚µã‚¤ã‚º = 4 * 8 = 32\n",
    "    learning_rate=3e-4,  # å°‘ã—é«˜ã‚ã®å­¦ç¿’ç‡ã§åŠ¹ç‡çš„ã«å­¦ç¿’\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=50,  # 50ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ­ã‚°å‡ºåŠ›\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),  # GPUä½¿ç”¨æ™‚ã¯FP16ã§é«˜é€ŸåŒ–\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",  # æ‰‹å‹•ã§ä¿å­˜\n",
    "    report_to=[],  # TensorBoardã¯ä½¿ã‚ãªã„ï¼ˆColabã§è¤‡é›‘ã«ãªã‚‹ãŸã‚ï¼‰\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ã‚«ã‚¹ã‚¿ãƒ Trainerï¼ˆlossè¿½è·¡ç”¨ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losså±¥æ­´ã‚’è¿½è·¡ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ Trainer\n",
    "class RevDEQTrainerWithHistory(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_history = []\n",
    "        self.step_history = []\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        if isinstance(outputs, dict):\n",
    "            loss = outputs.get(\"loss\")\n",
    "            if return_outputs:\n",
    "                return loss, outputs\n",
    "            return loss\n",
    "        elif isinstance(outputs, tuple):\n",
    "            logits, loss = outputs\n",
    "            if return_outputs:\n",
    "                return loss, {\"logits\": logits}\n",
    "            return loss\n",
    "        else:\n",
    "            if return_outputs:\n",
    "                return None, outputs\n",
    "            return None\n",
    "    \n",
    "    def log(self, logs):\n",
    "        # Lossã‚’è¨˜éŒ²\n",
    "        if \"loss\" in logs:\n",
    "            self.loss_history.append(logs[\"loss\"])\n",
    "            self.step_history.append(logs.get(\"step\", len(self.loss_history)))\n",
    "            \n",
    "            # 25ä»¥ä¸‹ã«ãªã£ãŸã‚‰é€šçŸ¥\n",
    "            if logs[\"loss\"] <= 25.0:\n",
    "                print(f\"\\nğŸ‰ Loss reached target! Current loss: {logs['loss']:.4f}\")\n",
    "        \n",
    "        super().log(logs)\n",
    "\n",
    "trainer = RevDEQTrainerWithHistory(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized with loss tracking!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. å­¦ç¿’ã®å®Ÿè¡Œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’é–‹å§‹\n",
    "print(\"=\" * 60)\n",
    "print(\"Starting training...\")\n",
    "print(f\"Target: Loss <= 25.0\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Lossã®å¯è¦–åŒ–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losså±¥æ­´ã®å¯è¦–åŒ–\n",
    "if len(trainer.loss_history) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Lossæ›²ç·š\n",
    "    steps = trainer.step_history if trainer.step_history else range(len(trainer.loss_history))\n",
    "    plt.plot(steps, trainer.loss_history, label='Training Loss', linewidth=2)\n",
    "    \n",
    "    # ç›®æ¨™ç·šï¼ˆLoss = 25ï¼‰\n",
    "    plt.axhline(y=25.0, color='r', linestyle='--', label='Target (Loss = 25)', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Step', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('RevDEQ Training Loss Progress', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # æœ€çµ‚çš„ãªlossã‚’è¡¨ç¤º\n",
    "    final_loss = trainer.loss_history[-1]\n",
    "    min_loss = min(trainer.loss_history)\n",
    "    \n",
    "    plt.text(0.02, 0.98, \n",
    "             f'Final Loss: {final_loss:.4f}\\n' +\n",
    "             f'Min Loss: {min_loss:.4f}\\n' +\n",
    "             f'Target: â‰¤25.0\\n' +\n",
    "             f'Status: {\"âœ… Achieved!\" if min_loss <= 25.0 else \"â³ Not yet\"}',\n",
    "             transform=plt.gca().transAxes,\n",
    "             fontsize=11,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # çµ±è¨ˆæƒ…å ±\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Loss Statistics:\")\n",
    "    print(f\"  Initial Loss: {trainer.loss_history[0]:.4f}\")\n",
    "    print(f\"  Final Loss: {final_loss:.4f}\")\n",
    "    print(f\"  Minimum Loss: {min_loss:.4f}\")\n",
    "    print(f\"  Loss Reduction: {trainer.loss_history[0] - min_loss:.4f}\")\n",
    "    print(f\"  Target (â‰¤25.0): {'âœ… Achieved!' if min_loss <= 25.0 else 'â³ Not yet - try more epochs or adjust hyperparameters'}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"No loss history recorded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "import torch\n",
    "\n",
    "save_dir = \"./checkpoints/final_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": model_config,\n",
    "}, os.path.join(save_dir, \"model.pt\"))\n",
    "\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"Model saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. æ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’ãƒ†ã‚¹ãƒˆ\n",
    "model.eval()\n",
    "\n",
    "test_prompts = [\n",
    "    \"The quick brown fox\",\n",
    "    \"In the beginning\",\n",
    "    \"Machine learning is\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_ids,\n",
    "            max_length=50,\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€RevDEQãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€lossãŒ25ç¨‹åº¦ã¾ã§ä¸‹ãŒã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚\n",
    "\n",
    "**ä¸»ãªãƒã‚¤ãƒ³ãƒˆ:**\n",
    "- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’Colabã§å®Ÿç”¨çš„ãªç¯„å›²ã«èª¿æ•´\n",
    "- è¤‡æ•°ã‚¨ãƒãƒƒã‚¯ã§ã®å­¦ç¿’ã§lossã‚’ä¸‹ã’ã‚‹\n",
    "- Losså±¥æ­´ã‚’å¯è¦–åŒ–ã—ã¦ç›®æ¨™é”æˆã‚’ç¢ºèª\n",
    "- ç›®æ¨™ï¼ˆloss â‰¤ 25ï¼‰ã«é”ã—ãŸå ´åˆã¯é€šçŸ¥\n",
    "\n",
    "**ã‚‚ã—lossãŒ25ã¾ã§ä¸‹ãŒã‚‰ãªã„å ´åˆ:**\n",
    "- ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã™ï¼ˆ`num_train_epochs`ã‚’å¢—ã‚„ã™ï¼‰\n",
    "- å­¦ç¿’ç‡ã‚’èª¿æ•´ï¼ˆ`learning_rate`ã‚’å¤‰æ›´ï¼‰\n",
    "- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å¤§ããã™ã‚‹ï¼ˆ`hidden_size`ã‚’å¢—ã‚„ã™ï¼‰\n",
    "- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}